{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\n",
    "\n",
    "This is relevant to our RAG application, which should maintain context from prior conversations with users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group traces into threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\n",
    "\n",
    "To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\n",
    "\n",
    "The key value is the unique identifier for that conversation. The key name should be one of:\n",
    "\n",
    "- session_id\n",
    "- thread_id\n",
    "- conversation_id.\n",
    "\n",
    "The value should be a UUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's run our application twice with this thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names \"thread_id,\" \"session_id,\" and \"conversation_id\" are preferred for metadata because they provide clear and specific identifiers for tracking conversations in a structured manner. Using these standardized names helps ensure consistency and makes it easier to filter and manage traces related to specific interactions. Additionally, they align with common practices in conversation tracking, enhancing interoperability across different systems.\n"
     ]
    }
   ],
   "source": [
    "question = \"Trace Number 1 for thread why is name thread_id or session_id or conversation_id preffered over other names for the metadata?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The UUID serves as a unique identifier for the trace, ensuring that each run can be distinctly recognized and tracked within the system. Using a UUID is better than a random value for the session_id because it guarantees uniqueness and avoids potential collisions, which can lead to confusion in trace management. This structured approach enhances the reliability and integrity of the tracing process.\n"
     ]
    }
   ],
   "source": [
    "question = \"Trace Number 2 for thread what is the puropse of the UUID and why is it better than giving any random value to the session_id?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-0 started | Task ID: 8f15db95-7d5a-41b9-be7b-760dbd71daba\n",
      "Thread-1 started | Task ID: 12010e73-4651-45f0-b605-29d3a5cd1572\n",
      "Thread-2 started | Task ID: 8fa10f3a-5770-41bf-b99b-a1377f725d07\n",
      "Thread-3 started | Task ID: ad3fb0da-ce7b-47d0-b1eb-3acc1558582f\n",
      "Thread-4 started | Task ID: c4b89704-8348-41e4-9da8-cc475e3f9a39\n",
      "Thread-0 finished | Task ID: 8f15db95-7d5a-41b9-be7b-760dbd71daba\n",
      "Thread-1 finished | Task ID: 12010e73-4651-45f0-b605-29d3a5cd1572\n",
      "Thread-2 finished | Task ID: 8fa10f3a-5770-41bf-b99b-a1377f725d07\n",
      "Thread-3 finished | Task ID: ad3fb0da-ce7b-47d0-b1eb-3acc1558582f\n",
      "Thread-4 finished | Task ID: c4b89704-8348-41e4-9da8-cc475e3f9a39\n",
      "All threads completed.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "def worker(thread_id):\n",
    "    # Generate a unique ID for this task\n",
    "    task_id = uuid.uuid4()\n",
    "    print(f\"Thread-{thread_id} started | Task ID: {task_id}\")\n",
    "    time.sleep(0.5)  # simulate some work\n",
    "    print(f\"Thread-{thread_id} finished | Task ID: {task_id}\")\n",
    "\n",
    "# Create and start multiple threads\n",
    "threads = []\n",
    "for i in range(5):  # launching 5 threads\n",
    "    t = threading.Thread(target=worker, args=(i,))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"All threads completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
